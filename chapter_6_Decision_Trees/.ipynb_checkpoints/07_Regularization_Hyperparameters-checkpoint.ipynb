{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f8ea1d2",
   "metadata": {},
   "source": [
    "# Regularization Hyperparameters\n",
    "\n",
    "Decision Trees make very few assumptions about the training data (as opposed to linear models, which obviously assume that the data is linear, for example).\n",
    "\n",
    "If left unconstrained, the tree structure will adapt itself to the training data, fitting it very\n",
    "closely, and most likely overfitting it. Such a model is often called a nonparametric model\n",
    "\n",
    "##### Nonparametric model:\n",
    "* the number of parameters is not determined prior to training, so the model structure is free to stick closely to the data.\n",
    "\n",
    "##### Parametric model:\n",
    "* has a predetermined number of parameters, so its degree of freedom is limited, reducing the risk of overfitting (but increasing the risk of underfitting)\n",
    "\n",
    "To avoid overfitting the training data, you need to restrict the Decision Tree’s freedom during training. As you know by now, this is called regularization. \n",
    "\n",
    "The regularization hyperparameters depend on the algorithm used, but generally you can at least restrict the maximum depth of the Decision Tree. \n",
    "\n",
    "In Scikit-Learn, this is controlled by the max_depth hyperparameter (the default value is None, which means unlimited).\n",
    "Reducing max_depth will regularize the model and thus reduce the risk of overfitting.\n",
    "\n",
    "The DecisionTreeClassifier class has a few other parameters that similarly restrict\n",
    "the shape of the Decision Tree:\n",
    "1. min_samples_split - the minimum number of samples a node must have before it can be split\n",
    "2. min_samples_leaf - the minimum number of samples a leaf node must have\n",
    "3. min_weight_fraction_leaf - same as min_samples_leaf but expressed as a fraction of the total number of weighted instances\n",
    "4. max_leaf_nodes - maximum number of leaf nodes\n",
    "5. max_features - maximum number of features that are evaluated for splitting at each node\n",
    "6. Increasing min_* hyperparameters or reducing max_* hyperparameters will regularize the model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcefd86d",
   "metadata": {},
   "source": [
    "Other algorithms work by first training the Decision Tree without restrictions, then pruning (deleting) unnecessary nodes. A node whose children are all leaf nodes is considered unnecessary if the purity improvement it provides is not statistically significant. Standard statistical tests, such as the χ2 test, are used to estimate the probability that the improvement is purely the result of chance (which is called the null hypothesis). If this probability, called the p-value, is higher than a given threshold (typically 5%, controlled by a hyperparameter), then the node is considered unnecessary and its children are deleted. The pruning continues until all unnecessary nodes have been pruned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f35caca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
