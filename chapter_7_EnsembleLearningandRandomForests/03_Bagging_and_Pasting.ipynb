{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9694816e",
   "metadata": {},
   "source": [
    "# Bagging and Pasting\n",
    "\n",
    "Use the same training algorithm for every predictor, but to train them on different random subsets of the training set\n",
    "\n",
    "##### Bagging:\n",
    "When sampling is performed with replacement\n",
    "##### Pasting:\n",
    "When sampling is performed without replacement\n",
    "\n",
    "In other words, both bagging and pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor.\n",
    "\n",
    "This sampling and training process is represented in Figure 7-4.\n",
    "\n",
    "<img src='img_4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e754f43",
   "metadata": {},
   "source": [
    "Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors. \n",
    "\n",
    "*statistical mode*: The aggregation function. (i.e., the most frequent prediction, just like a\n",
    "hard voting classifier for classification, or the average for regression)\n",
    "\n",
    "Each individual predictor has a higher bias than if it were trained on the original training set, but\n",
    "aggregation reduces both bias and variance.\n",
    "\n",
    "Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the\n",
    "original training set.\n",
    "\n",
    "predictors can all be trained in parallel, via different CPU cores or even different servers. This is one of the reasons why bagging and pasting are such popular methods: they scale very well.\n",
    "\n",
    "## Bagging and Pasting in Scikit-Learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa399750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e9fbed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data[:, 2:], data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "293be492",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(base_estimator= DecisionTreeClassifier(), n_estimators = 500, \n",
    "                           max_samples=100, bootstrap=True, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5127248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=100,\n",
       "                  n_estimators=500, n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BaggingClassifier</label><div class=\"sk-toggleable__content\"><pre>BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=100,\n",
       "                  n_estimators=500, n_jobs=-1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">base_estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=100,\n",
       "                  n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b17e648",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235b8e39",
   "metadata": {},
   "source": [
    "The BaggingClassifier automatically performs soft voting instead of hard voting if the base classifier can estimate class probabilities (i.e., if it has a predict_proba() method), which is the case with Decision Trees classifiers.\n",
    "\n",
    "<img src='img_5.png'>\n",
    "\n",
    "As you can see, the ensemble’s predictions will likely generalize much better than the single Decision Tree’s predictions: the ensemble has a comparable bias but a smaller variance (it makes roughly the same number of errors on the training set, but the decision boundary is less irregular)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31801891",
   "metadata": {},
   "source": [
    "Overall, bagging often results in better models, which explains why it is generally preferred. However, if you have spare time and CPU power you can use cross-validation to evaluate both bagging and pasting and select the one that works best.\n",
    "\n",
    "Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slightly higher bias than pasting, but this also means that predictors end up being less correlated so the ensemble’s variance is reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671cb8bb",
   "metadata": {},
   "source": [
    "## Out-of-Bag Evaluation\n",
    "\n",
    "With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all.\n",
    "\n",
    "m: the size of training set\n",
    "\n",
    "By default a BaggingClassifier samples m training instances with replacement (bootstrap=True).\n",
    "This means that only about 63% of the training instances are sampled on average for each predictor:\n",
    "as m growth, the ratio become:\n",
    "\n",
    "\n",
    "1 - $e^{-1}$ $\\approx$ 63%\n",
    "\n",
    "The remaining 37% of the training instances that are not sampled are called out-of-bag (oob) instances\n",
    "\n",
    "*out-of-bag* intances: are the training instances that are not sampled \n",
    "\n",
    "\n",
    "Since a predictor never sees the oob instances during training, it can be evaluated on these instances, without the need for a separate validation set or cross-validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "046ada11",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf_oob = BaggingClassifier(DecisionTreeClassifier(), oob_score=True, \n",
    "                                n_estimators = 500, max_samples=100, \n",
    "                                bootstrap=True, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1853e453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9642857142857143"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf_oob.fit(X_train, y_train)\n",
    "bag_clf_oob.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94b2484e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(bag_clf_oob.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a7dc791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.14027149, 0.85972851],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.98173516, 0.01826484],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.16402116, 0.83597884],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.05882353, 0.94117647],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.37558685, 0.62441315],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.86666667, 0.13333333],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.8       , 0.2       ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.71134021, 0.28865979],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.88324873, 0.11675127],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.41964286, 0.58035714],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.97607656, 0.02392344],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.09345794, 0.90654206],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.87317073, 0.12682927],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.2712766 , 0.7287234 ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.11904762, 0.88095238],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.05555556, 0.94444444],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.97619048, 0.02380952],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.01395349, 0.98604651],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf_oob.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570ec4c6",
   "metadata": {},
   "source": [
    "The oob decision function for each training instance is also available through the\n",
    "oob_decision_function_ variable. In this case (since the base estimator has a pre\n",
    "dict_proba() method) the decision function returns the class probabilities for each\n",
    "training instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abc7478d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.14027149, 0.85972851])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf_oob.oob_decision_function_[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d929113b",
   "metadata": {},
   "source": [
    "For example, the oob evaluation estimates that the first training instance has a 85.97% probability of belonging to the $1^{st}$ class (and 14.02% of belonging to the $2^{nd}$ class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91449dbd",
   "metadata": {},
   "source": [
    "## Random Patches and Random Subspaces\n",
    "\n",
    "The BaggingClassifier class supports sampling the features as well\n",
    "his is con‐\n",
    "trolled by two hyperparameters: \n",
    "1. max_features and \n",
    "2. bootstrap_features\n",
    "\n",
    "They work the same way as max_samples and bootstrap, but for feature sampling instead of instance sampling. Thus, each predictor will be trained on a random subset of the input features.\n",
    "\n",
    "\n",
    "This is particularly useful when you are dealing with high-dimensional inputs (such as images).\n",
    "\n",
    "*Random Patches Method*: Sampling both training instances and features.\n",
    "\n",
    "*Random Subspaces Method*: Keeping all training instances( bootstrap=False and max_samples=1.0) but sampling features (bootstrap_features=True and/or max_features smaller than 1.0)\n",
    "\n",
    "Sampling features results in even more predictor diversity, trading a bit more bias for a lower variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11a3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
