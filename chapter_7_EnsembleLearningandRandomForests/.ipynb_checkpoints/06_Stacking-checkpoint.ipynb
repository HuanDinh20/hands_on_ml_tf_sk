{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd859752",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "\n",
    "Ensemble method ~ Stacking ~ Stacked Generallization\n",
    " \n",
    "It is based on a simple idea: instead of using trivial functions(such as hard voting) to aggregate the predictions of all predictors in an ensemble, why don’t we train a model to perform this aggregation?\n",
    "\n",
    "<img src='img_16.png' >\n",
    "\n",
    " Figure 7-12 shows such an ensemble performing a regression task on a new instance. Each of the bottom three\n",
    "predictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor (called a blender, or a meta learner) takes these predictions as inputs and makes the final prediction (3.0).\n",
    "\n",
    "To train the blender, a common approach is to use a hold-out set.\n",
    "\n",
    "<img src='img_17.png' >\n",
    "\n",
    " First, the training set is split in two subsets. The first subset is used to train the\n",
    "predictors in the first layer (see Figure 7-13). \n",
    "\n",
    "<img src='img_18.png' >\n",
    "\n",
    "Next, the first layer predictors are used to make predictions on the second (held-out) set (see Figure 7-14). This ensures that the predictions are “clean,” since the predictors never saw these instances during training.\n",
    "\n",
    "Now for each instance in the hold-out set there are three predicted values. We can create a new training set using these predicted values as input features (which makes this new training set three-dimensional), and keeping the target values.\n",
    " the second one is used to create the training set used to\n",
    "The blender is trained on this new training set, so it learns to predict the target value given the first layer’s predictions.\n",
    "\n",
    "<img src='img_19.png' >\n",
    "\n",
    "It is actually possible to train several different blenders this way (e.g., one using Linear Regression, another using Random Forest Regression, and so on): we get a whole layer of blenders.\n",
    "\n",
    "The trick is to split the training set into three subsets: \n",
    "1. The first one is used to train the first layer\n",
    "2. The second one is used to create the training set used to train the second layer (using predictions made by the predictors of the first layer)\n",
    "3. The third one is used to create the training set to train the third layer\n",
    "\n",
    " Once this is done, we can make a prediction for a new instance by going through each layer sequentially.\n",
    " \n",
    " Unfortunately, Scikit-Learn does not support stacking directly, but it is not too hard to roll out your own implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48aa818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
